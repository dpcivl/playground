# 🤔 뉴런의 개수는 어떻게 정할까?
MNIST 예제를 수행하다가 첫번째 레이어의 뉴런의 개수를 정하다가 제목과 같은 생각이 들었다.  
뉴런의 개수를 정하는 규칙은 딱히 없지만, 보통 입력 데이터보다 작게 뉴런의 개수를 정한다고 했다. MNIST 예제의 경우 28*28 = 784이기 때문에, 784보다 작은 512, 256 등의 개수로 뉴런 개수를 정한다고 한다.  

# ✅ 이미지 데이터 평탄화(Flattening), 정규화(Normalization)
```python
train_images = train_images.reshape(train_images.shape[0], -1) / 255.0
```
🔼 `Dense` 레이어를 사용하기 위해서는 위와 같이 이미지 평탄화가 필요하다.  
또한 정규화를 해주기 위해서 `255.0`으로 나눠준다.  
정규화를 해주는 이유는 이미지의 각 픽셀 값을 0과 1사이의 범위로 스케일링하기 위함인데, 이 작업을 통해서 데이터를 처리하기 쉬운 형태로 변환된다고 한다.  

## 🤔 왜 정수보다 부동소수점 연산이 처리하기 쉽지?
- 수치 안정성 : 입력 데이터의 스케일이 너무 크거나 작으면 최적화하기 어려움
- 학습 효율성 : 모든 입력 특성을 동일한 스케일로 정규화하면, 가중치 초기화와 그레디언트 업데이트가 더 효율적으로 이루어짐
- 모델 일반화 : 정수 데이터뿐만 아니라, 다양한 데이터에 대해 훈련하기 때문에 좋은 성능을 기대할 수 있음
- 활성화 함수 : 신경망에서 사용하는 비선형 활성화 함수가 더 잘 작동하기 위한 범위 내로 스케일링 하기 때문

# ✅ `model.compile()`의 역할
`compile()` 메소드는 학습하기 전에 학습 환경을 구성하는 역할이다.  
## optimizer
모델의 업데이트 방법, 즉 가중치를 어떻게 조정할지 결정하는 알고리즘을 정의한다.  
## loss
손실 함수를 정의한다.  
## metrics
훈련과 테스트 과정을 모니터링하기 위해 사용되는 평가 지표를 정의한다.  

# 😅 validation이 왜 필요하더라..??
학습 과정에서 따로 validation set을 빼서 학습하지 않은 데이터에 대해 얼마나 잘 일반화되는지 평가한다.  
`validation_rate`가 0.2로 설정되어 있다면, 전체 학습 데이터 중 20퍼센트가 validation에 사용된다.  
각 epoch마다 validation을 사용하고, 검증 손실이나 검증 정확도 값을 표출해서 모델이 제대로 학습하고 있는지 모니터링 할 수 있도록 해준다.👍